{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicode_data  # Optional, for normalization if desired\n",
    "import torch\n",
    "\n",
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        # The initial vocabulary consists of the 256 bytes (0–255)\n",
    "        self.merges = {}  # (int, int) -> int\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\"\n",
    "        Trains the tokenizer by finding the most frequent byte pairs\n",
    "        and merging them until the desired vocabulary size is reached.\n",
    "        \"\"\"\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        text_bytes = text.encode(\"utf-8\")  # Convert to UTF-8 bytes\n",
    "        ids = list(text_bytes)  # List of integers (0–255)\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            # 1. Count frequencies of consecutive pairs\n",
    "            stats = self._get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            # 2. Find the most frequent pair\n",
    "            top_pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "            \n",
    "            # 3. Register the merge and update the vocabulary\n",
    "            if verbose:\n",
    "                print(f\"Merging {top_pair} into new token {idx}\")\n",
    "            \n",
    "            self.merges[top_pair] = idx\n",
    "            self.vocab[idx] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
    "            \n",
    "            # 4. Replace the pair in the ID sequence\n",
    "            ids = self._merge(ids, top_pair, idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Converts text into a list of tokens (IDs).\"\"\"\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            stats = self._get_stats(ids)\n",
    "            # Find the pair that appears in our trained merges with the smallest index\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break  # No more merges possible\n",
    "            idx = self.merges[pair]\n",
    "            ids = self._merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"Converts a list of IDs back into text (string).\"\"\"\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        # Use 'replace' to handle malformed byte sequences\n",
    "        return text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def _get_stats(self, ids):\n",
    "        counts = {}\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def _merge(self, ids, pair, idx):\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and (ids[i], ids[i + 1]) == pair:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage ---\n",
    "tokenizer = BasicTokenizer()\n",
    "corpus = \"This is an example text to train the tokenizer, like in Karpathy's video.\"\n",
    "tokenizer.train(corpus, vocab_size=260, verbose=True)\n",
    "\n",
    "tokens = tokenizer.encode(\"Hello Karpathy\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: {tokenizer.decode(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#pip install tiktoken\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. Configurar el tokenizador (o200k_base para GPT-4o, cl100k_base para GPT-4/3.5)\u001b[39;00m\n\u001b[32m      6\u001b[39m encoding = tiktoken.get_encoding(\u001b[33m\"\u001b[39m\u001b[33mo200k_base\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "# pip install tiktoken\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# 1. Configure the tokenizer (o200k_base for GPT-4o, cl100k_base for GPT-4/3.5)\n",
    "encoding = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "text = \"Hola, probando el tokenizador de OpenAI en Google Colab.\"\n",
    "\n",
    "# 2. Encode text into tokens\n",
    "tokens = encoding.encode(text)\n",
    "\n",
    "# 3. Display results\n",
    "print(f\"Token IDs: {tokens}\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Recovered text: {encoding.decode(tokens)}\")\n",
    "\n",
    "# View breakdown of individual tokens\n",
    "print(\"Breakdown:\", [encoding.decode_single_token_bytes(t) for t in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices\n",
    "\n",
    "Modify the tokenizer class to include the regex for splitting words before performing BPE. (Original: Modificar la clase tokenizer para que incluya la regex para dividir palabras antes de hacer BPE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class BasicTokenizer:\n",
    "    def __init__(self):\n",
    "        # Initial vocabulary: 256 bytes\n",
    "        self.merges = {}              # (int, int) -> int\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "        # GPT-style regex for pre-tokenization\n",
    "        self.pattern = re.compile(\n",
    "            r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Training (regex → byte-level BPE)\n",
    "    # ------------------------------------------------------------------\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # Regex split\n",
    "        words = self.pattern.findall(text)\n",
    "\n",
    "        # Convert to byte ids\n",
    "        ids = []\n",
    "        for w in words:\n",
    "            ids.extend(list(w.encode(\"utf-8\")))\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = 256 + i\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Merging {pair} -> {idx}\")\n",
    "\n",
    "            self.merges[pair] = idx\n",
    "            self.vocab[idx] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "            ids = self._merge(ids, pair, idx)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Encoding / decoding\n",
    "    # ------------------------------------------------------------------\n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "\n",
    "        words = self.pattern.findall(text)\n",
    "        for w in words:\n",
    "            ids = list(w.encode(\"utf-8\"))\n",
    "\n",
    "            while len(ids) >= 2:\n",
    "                stats = self._get_stats(ids)\n",
    "                pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "                if pair not in self.merges:\n",
    "                    break\n",
    "                ids = self._merge(ids, pair, self.merges[pair])\n",
    "\n",
    "            tokens.extend(ids)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return b\"\".join(self.vocab[i] for i in ids).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Helpers\n",
    "    # ------------------------------------------------------------------\n",
    "    def _get_stats(self, ids):\n",
    "        counts = {}\n",
    "        for a, b in zip(ids, ids[1:]):\n",
    "            counts[(a, b)] = counts.get((a, b), 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def _merge(self, ids, pair, idx):\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage ---\n",
    "tokenizer = BasicTokenizer()\n",
    "corpus = \"This is an example text to train the tokenizer, like in Karpathy's video.\"\n",
    "tokenizer.train(corpus, vocab_size=260, verbose=True)\n",
    "\n",
    "tokens = tokenizer.encode(\"Hello Karpathy\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: {tokenizer.decode(tokens)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
