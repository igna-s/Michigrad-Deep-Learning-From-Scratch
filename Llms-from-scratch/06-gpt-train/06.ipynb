{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "73a59a39-98b5-4b55-b29a-33e52f5adcfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73a59a39-98b5-4b55-b29a-33e52f5adcfa",
        "outputId": "4d8c296b-aaa3-4f1e-91a3-02ac603b8809"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a57684958b0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tiktoken as tk\n",
        "\n",
        "# using gpt2 encoding this time\n",
        "encoder = tk.get_encoding('gpt2')\n",
        "\n",
        "# ----- Hyperparameters\n",
        "# setting up all the config numbers here\n",
        "batch_size = 64      # how many sequences to process in parallel\n",
        "block_size = 64      # max context length\n",
        "max_iters = 4000     # total training steps\n",
        "eval_interval = 100  # how often to check loss\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200     # how many steps to average for loss estimation\n",
        "# check if cuda is available to speed things up\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2        # to prevent overfitting\n",
        "# ------\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0290779d-4dd7-44df-89c3-e79525fe6f6b",
      "metadata": {
        "id": "0290779d-4dd7-44df-89c3-e79525fe6f6b"
      },
      "outputs": [],
      "source": [
        "# reading the domain names file\n",
        "with open('the_art_of_war.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "# vocab_size =  len(chars)\n",
        "# using the size from the encoder directly\n",
        "vocab_size =  encoder.n_vocab\n",
        "\n",
        "# old manual encoding (commented out)\n",
        "#stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "#itos = {i:ch for i, ch in enumerate(chars)}\n",
        "#encode = lambda s:[stoi[c] for c in s]\n",
        "#decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# creating the mappings using the encoder\n",
        "stoi = {encoder.decode([k]):k for k in range(encoder.n_vocab)}\n",
        "itos = {k:encoder.decode([k]) for k in range(encoder.n_vocab)}\n",
        "encode = encoder.encode\n",
        "decode = encoder.decode\n",
        "\n",
        "# encoding the whole dataset into a tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# 90% train, 10% val split\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "869e326f-e43c-4bfd-bf19-d92a0cf32fe0",
      "metadata": {
        "id": "869e326f-e43c-4bfd-bf19-d92a0cf32fe0"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    # helper to grab a random chunk of data\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    # generate random starting spots\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    # stack them up\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    # move to gpu if available\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    # function to calculate loss without updating gradients (for eval)\n",
        "    out = {}\n",
        "    model.eval() # switch to eval mode\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train() # switch back to train mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee15bf00-e9ea-499c-8df8-5de64c6bb07a",
      "metadata": {
        "id": "ee15bf00-e9ea-499c-8df8-5de64c6bb07a"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    \"\"\" One single head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # key, query, value projections\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # buffer for the mask so it's not treated as a parameter\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C **-.5\n",
        "        # apply the mask (so tokens can't see the future)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        # aggregate the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class AttentionMultiHead(nn.Module):\n",
        "    \"\"\" Multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # creating a list of heads\n",
        "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # concatenate the results from all heads\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # scaling up by 4 is standard\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # scaling back down\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" Putting it all together: communication + computation \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = AttentionMultiHead(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        # layer norms\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # adding residual connections\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "13c92dbe-880a-4a2a-8145-70c8e0cc74a9",
      "metadata": {
        "id": "13c92dbe-880a-4a2a-8145-70c8e0cc74a9"
      },
      "outputs": [],
      "source": [
        "class DomainGeneratorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # embedding for the tokens\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # embedding for the positions\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        # sequence of transformer blocks\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        # language model head to map back to vocab size\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # positional info\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # reshaping to fit cross_entropy expects\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop context so it doesn't exceed block_size\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cc1d11b2-1840-421e-9809-768f851708b1",
      "metadata": {
        "id": "cc1d11b2-1840-421e-9809-768f851708b1"
      },
      "outputs": [],
      "source": [
        "# creating the model instance\n",
        "model = DomainGeneratorModel()\n",
        "model = model.to(device)\n",
        "\n",
        "# using AdamW optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "45ebe4db-747b-454e-957c-9ed75ee633b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45ebe4db-747b-454e-957c-9ed75ee633b9",
        "outputId": "559f1e2f-04ea-4ea6-ccb6-20940742dfc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 6.686545 M parameters\n"
          ]
        }
      ],
      "source": [
        "# checking how big the model is (millions of params)\n",
        "print(f'Model has {sum(p.numel() for p in model.parameters())/1e6} M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b184dd7a-caea-4fe5-86e8-cb7c0774c51b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b184dd7a-caea-4fe5-86e8-cb7c0774c51b",
        "outputId": "c2ea2254-5a78-4dc7-a6aa-57e37df54791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step     0: train loss: 11.2993, val loss: 11.3019\n",
            "step   100: train loss: 6.0323, val loss: 6.4704\n",
            "step   200: train loss: 5.4485, val loss: 6.1600\n",
            "step   300: train loss: 5.0020, val loss: 6.0839\n",
            "step   400: train loss: 4.6506, val loss: 6.0887\n",
            "step   500: train loss: 4.3612, val loss: 6.1274\n",
            "step   600: train loss: 4.1199, val loss: 6.1505\n",
            "step   700: train loss: 3.9346, val loss: 6.2082\n",
            "step   800: train loss: 3.7731, val loss: 6.2408\n",
            "step   900: train loss: 3.6382, val loss: 6.2529\n",
            "step  1000: train loss: 3.5224, val loss: 6.2898\n",
            "step  1100: train loss: 3.4105, val loss: 6.3326\n",
            "step  1200: train loss: 3.3014, val loss: 6.3576\n",
            "step  1300: train loss: 3.2012, val loss: 6.3960\n",
            "step  1400: train loss: 3.1073, val loss: 6.4542\n",
            "step  1500: train loss: 3.0268, val loss: 6.5095\n",
            "step  1600: train loss: 2.9280, val loss: 6.5829\n",
            "step  1700: train loss: 2.8539, val loss: 6.6247\n",
            "step  1800: train loss: 2.7732, val loss: 6.6806\n",
            "step  1900: train loss: 2.6918, val loss: 6.7254\n",
            "step  2000: train loss: 2.6074, val loss: 6.7773\n",
            "step  2100: train loss: 2.5355, val loss: 6.8639\n",
            "step  2200: train loss: 2.4625, val loss: 6.9134\n",
            "step  2300: train loss: 2.3985, val loss: 6.9380\n",
            "step  2400: train loss: 2.3485, val loss: 7.0201\n",
            "step  2500: train loss: 2.2822, val loss: 7.0378\n",
            "step  2600: train loss: 2.2258, val loss: 7.1242\n",
            "step  2700: train loss: 2.1751, val loss: 7.1526\n",
            "step  2800: train loss: 2.1223, val loss: 7.2175\n",
            "step  2900: train loss: 2.0656, val loss: 7.2402\n",
            "step  3000: train loss: 2.0223, val loss: 7.2906\n",
            "step  3100: train loss: 1.9816, val loss: 7.3071\n",
            "step  3200: train loss: 1.9377, val loss: 7.3426\n",
            "step  3300: train loss: 1.9017, val loss: 7.4291\n",
            "step  3400: train loss: 1.8557, val loss: 7.4458\n",
            "step  3500: train loss: 1.8278, val loss: 7.4305\n",
            "step  3600: train loss: 1.7843, val loss: 7.5095\n",
            "step  3700: train loss: 1.7508, val loss: 7.5579\n",
            "step  3800: train loss: 1.7066, val loss: 7.5928\n",
            "step  3900: train loss: 1.6746, val loss: 7.6270\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "for step in range(max_iters):\n",
        "\n",
        "    # every now and then, evaluate loss on train and val sets\n",
        "    if step % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f'step {step:5d}: train loss: {losses[\"train\"]:.4f}, val loss: {losses[\"val\"]:.4f}')\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # clear gradients before backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "aaed933d-8b2a-4a55-860e-c02df3663191",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaed933d-8b2a-4a55-860e-c02df3663191",
        "outputId": "f2397aac-4f03-4213-c001-afbd41132f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test generation >>>>>>>>>\n",
            "!\n",
            "\"When as a different œuver forbut, and Tao-ch’en as Sun Tz\n",
            "appears that the Duke, we possess from that its battle had through the Chin\n",
            "so thoroughly investigated before he who were cowards the enemy’s in\n",
            "preceded a generality in the scale-tengou-fu city text Wang, was further\n",
            "a permit in order to seize an interjumping. It is been\n",
            "53. They may not yet reached through the aid of Greek, but when\n",
            "to see within a seems to walled cities (to the enemy’s rear, worn-up\n",
            "emotion. § 7], char the energy, regard this will be an end, however, KLi Ch’uan and\n",
            "\n",
            "[Ch’uan says: \"plortioning our camp thus we are the unwise\n",
            "ure their number way and canonized by one paraphrase which had been\n",
            "city of Ying. When his horse, nowpt into Tu Mu, and\n",
            "himself. Ching fighting appears to him out of the two generals to write;\n",
            "and the Emperor to return in 50 afar\n",
            "chance of strategy the drum\n",
            "explanation, but occurs later on disp commands are too much that the enemy’s greatest\n",
            "whose language. Ts’ao Kung’ao Kung’s death, himself; when equally plausible explanation’s\n",
            "out the remaining ring place unmethods hands, without supplies, and the\n",
            "witnessing them.\"\n",
            "gongs and plundering or Bonaparte?\"ily\n",
            "\n",
            "\n",
            "35. On serious-mThe rules are certain of this\" VI. thinks, note, however, Wu is\n",
            "however, shows that he does not were reduced to be in fits hasty temper, but\n",
            "all the barbarians in order to be taken to further advantage before a hard pressedbow;\n",
            "\n",
            "too, when distinct;\n",
            "much more thoroughly, but also disadvantageous times, is therefore be prepared, we can\n",
            "surely that our opponents may be divided the enemy will be allowed to attack from\n",
            "s first of a close watch on an army on.\n",
            "\n",
            "\n",
            "[Li Ch’ which also also quote here the \" they die.\" I asked Jan Yu\n",
            "\" hardly taken to be found the King of Ch’in Hao-lin quotes Ch’ao night ride \"How large numbers have\n",
            "corn!\n",
            "<<<<<<<<<<<<<<<<< END\n"
          ]
        }
      ],
      "source": [
        "print('Test generation >>>>>>>>>')\n",
        "# start with an empty context (or whatever token 0 is)\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "# generate 500 tokens and print the result\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "print('<<<<<<<<<<<<<<<<< END')\n",
        "\n",
        "# saving the model for later\n",
        "torch.save(model, 'models/my_first_TLM.torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1298da9a-5226-45a9-b71d-ce482e5f97b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1298da9a-5226-45a9-b71d-ce482e5f97b5",
        "outputId": "32377f51-79c3-4ba5-df1b-685c0b605ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has 6.686545 M parameters\n"
          ]
        }
      ],
      "source": [
        "# loading a previously saved model (just testing load functionality)\n",
        "\n",
        "model = torch.load(\n",
        "    'models/my_first_TLM.torch',\n",
        "    map_location=device,\n",
        "    weights_only=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(f'Model has {sum(p.numel() for p in model.parameters())/1e6} M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3vRMy7HU7g",
        "outputId": "1bd5668c-d4e2-43a8-e9ff-96fd650345ef"
      },
      "id": "Ku3vRMy7HU7g",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DomainGeneratorModel(\n",
              "  (token_embedding_table): Embedding(50257, 64)\n",
              "  (position_embedding_table): Embedding(64, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (sa): AttentionMultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x AttentionHead(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (sa): AttentionMultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x AttentionHead(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (sa): AttentionMultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x AttentionHead(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (sa): AttentionMultiHead(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x AttentionHead(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=64, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test generation >>>>>>>>>')\n",
        "# start with an empty context (or whatever token 0 is)\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "# generate 500 tokens and print the result\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "print('<<<<<<<<<<<<<<<<< END')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd599UJTHZMs",
        "outputId": "6ef75433-768e-4618-e830-3cae0323cb6f"
      },
      "id": "rd599UJTHZMs",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test generation >>>>>>>>>\n",
            "!\n",
            "\n",
            "23. To refrain from the right river,\n",
            "\n",
            "32. When the left wing will be so as when the\n",
            "subject of fortified cities of its spirit throughout retreat, the right moment’s best allyander;\n",
            "\n",
            "[This is lacking in one. Conceal Sun Tzŭ went mentioned in ch. 2, his contemporaries Yuan and when about on\n",
            "the sure that the second are a mere string of the number of a native of above\n",
            "of the Chou Ch’in. Although all the year [26]\n",
            "I should say said:’ chapter deeply on the topmost heights and\n",
            "treacherous rival P’ang Yu’ing in 73\n",
            "doncriveadversary on their position, however, \"_esprit de corps_ he are\n",
            "in the same XIII. IV and ate the Chou [13] there is given in § 24, 2]. There\n",
            "is inclined to see that period immediately my equal to the two are rougher\n",
            "task. My contemporaries-thirds of the Chinese\n",
            "corruption in harmony in the last moment shows about to contemptuous\n",
            "which when it is strong and be achieved.\n",
            "\n",
            "\n",
            "\n",
            "20. He who knows is skilful general rule, the result should be if you\n",
            "with the leaders of the deepestaff front and look upon it\n",
            "before the position of the Empire which laid Italy at\n",
            "some of their time, by the army through their allies is surrounded by\n",
            "so thoroughly versed from every method, will who.\n",
            "\n",
            "[Tu Yu quotes the slightest claim to\n",
            "against Ch’eng Liao_, saying has Wei Liao T’ien had a deaf ear. P’ing in Lu Meng Ta had\n",
            "overpowered a light boat and canon of the study, by the\n",
            "field by Ch’i-restige of Ch’u, King’u is dissuaded military genius\n",
            "the part of the drum tends to match. This is the first some\n",
            "special diversion of the Japanese collaborators. My first Han, the similar being fissimo’s note is says: \"Powerful generalful in them\n",
            "already the loss of campaign. A hostile state, a few now come to follow him helpless. Much elated here the idea of our\n",
            "safety.]\n",
            "\n",
            "\n",
            "11. If your soldiers will follow you march.\n",
            "\n",
            "[Literally, Yao-ch’en understands our general, to make to\n",
            "<<<<<<<<<<<<<<<<< END\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}